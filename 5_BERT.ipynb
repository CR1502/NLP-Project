{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### While checking the code for grading you dont need to run this code, all the files required are uploaded to gogle drive and the link is shared as a text file in the main branch under the name Downloading required files, as it could not be uploaded manually or by github desktop.\n",
    "#### This file is for Training the BERT model. This outputs a few files that are required by 6_BERT_Test.py and 7_final_ensemble.py"
   ],
   "id": "77c7fa912955358f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T01:54:18.202260Z",
     "start_time": "2025-04-08T01:54:17.829573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code required for my system as I was facing issues with NLTK\n",
    "import os\n",
    "import ssl\n",
    "import nltk\n",
    "\n",
    "# Set the NLTK_DATA environment variable to your provided path\n",
    "os.environ[\"NLTK_DATA\"] = \"NLP-Project/nltk_data\"\n",
    "\n",
    "# Add this directory to nltk's search path\n",
    "nltk.data.path.append(\"NLP-Project/nltk_data\")\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"Setup complete. NLTK data path set to:\", os.environ[\"NLTK_DATA\"])"
   ],
   "id": "4459c5e3e560c563",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. NLTK data path set to: /Users/craigroberts/Documents/Coding/NLP/MediScan_NLP_Proj/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/craigroberts/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/craigroberts/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T01:54:20.106935Z",
     "start_time": "2025-04-08T01:54:18.205590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "1022d179506dc306",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T01:54:20.404890Z",
     "start_time": "2025-04-08T01:54:20.164723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"Final_data/train_data.csv\")  # Assumes columns 'claim' and 'label'\n",
    "dev_df   = pd.read_csv(\"Final_data/dev_data.csv\")\n",
    "\n",
    "print(\"Train DataFrame shape:\", train_df.shape)\n",
    "print(\"Dev DataFrame shape:\", dev_df.shape)\n",
    "\n",
    "# Encode labels if they are strings\n",
    "if train_df[\"label\"].dtype == object:\n",
    "    le = LabelEncoder()\n",
    "    all_labels = pd.concat([train_df[\"label\"], dev_df[\"label\"]], axis=0)\n",
    "    le.fit(all_labels)\n",
    "    train_df[\"label_encoded\"] = le.transform(train_df[\"label\"])\n",
    "    dev_df[\"label_encoded\"] = le.transform(dev_df[\"label\"])\n",
    "    num_labels = len(le.classes_)\n",
    "else:\n",
    "    train_df[\"label_encoded\"] = train_df[\"label\"]\n",
    "    dev_df[\"label_encoded\"] = dev_df[\"label\"]\n",
    "    num_labels = len(np.unique(train_df[\"label\"]))\n",
    "    class DummyLE:\n",
    "        pass\n",
    "    le = DummyLE()\n",
    "    le.classes_ = np.sort(np.unique(train_df[\"label\"]))\n",
    "\n",
    "print(\"Number of classes:\", num_labels)\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, range(num_labels))))\n",
    "\n",
    "# Create Hugging Face Datasets using 'claim' and 'label_encoded'\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"claim\", \"label_encoded\"]])\n",
    "dev_dataset = Dataset.from_pandas(dev_df[[\"claim\", \"label_encoded\"]])\n",
    "\n",
    "# Rename label column to \"labels\"\n",
    "train_dataset = train_dataset.rename_column(\"label_encoded\", \"labels\")\n",
    "dev_dataset = dev_dataset.rename_column(\"label_encoded\", \"labels\")\n",
    "\n",
    "# Remove extraneous columns (keep only 'claim' and 'labels')\n",
    "cols_to_keep = [\"claim\", \"labels\"]\n",
    "train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col not in cols_to_keep])\n",
    "dev_dataset = dev_dataset.remove_columns([col for col in dev_dataset.column_names if col not in cols_to_keep])\n",
    "\n",
    "print(\"Training samples:\", len(train_dataset), \"Dev samples:\", len(dev_dataset))"
   ],
   "id": "aa92589c75a9305a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame shape: (5338, 6)\n",
      "Dev DataFrame shape: (2224, 6)\n",
      "Number of classes: 2\n",
      "Label mapping: {np.int64(0): 0, np.int64(1): 1}\n",
      "Training samples: 5338 Dev samples: 2224\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T01:54:20.842510Z",
     "start_time": "2025-04-08T01:54:20.407157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the pre-trained BERT tokenizer (\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"claim\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "dev_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ],
   "id": "d4deebd1a827bb24",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5338/5338 [00:00<00:00, 38005.25 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2224/2224 [00:00<00:00, 42526.44 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T01:54:21.616708Z",
     "start_time": "2025-04-08T01:54:20.851941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = np.mean(preds == labels)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Base training arguments\n",
    "base_training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Load a fresh BERT model instance\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=base_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Trainer is set up!\")"
   ],
   "id": "6cc30de24095ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer is set up!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T03:07:25.621159Z",
     "start_time": "2025-04-08T01:54:21.632689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "# Define grid for hyperparameters -> to find the best parameters, better than randomly trying to get the best pair\n",
    "learning_rates = [2e-5, 3e-5]\n",
    "train_batch_sizes = [16, 32]\n",
    "eval_batch_sizes = [16, 32]\n",
    "\n",
    "best_acc = 0\n",
    "best_config = None\n",
    "results = []\n",
    "\n",
    "for lr, train_bs, eval_bs in itertools.product(learning_rates, train_batch_sizes, eval_batch_sizes):\n",
    "    print(f\"Training with lr={lr}, train_bs={train_bs}, eval_bs={eval_bs}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./temp_bert_output\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=train_bs,\n",
    "        per_device_eval_batch_size=eval_bs,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_dir=\"./temp_logs\",\n",
    "        logging_steps=50,\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Load a new model instance for each run\n",
    "    temp_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "    temp_model.to(device)\n",
    "\n",
    "    temp_trainer = Trainer(\n",
    "        model=temp_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    temp_trainer.train()\n",
    "    eval_results = temp_trainer.evaluate()\n",
    "    acc = eval_results[\"eval_accuracy\"]\n",
    "    results.append({\"learning_rate\": lr, \"train_bs\": train_bs, \"eval_bs\": eval_bs, \"accuracy\": acc})\n",
    "    print(f\"Configuration: lr={lr}, train_bs={train_bs}, eval_bs={eval_bs} -> Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_config = (lr, train_bs, eval_bs)\n",
    "\n",
    "print(\"Best configuration:\", best_config, \"with accuracy:\", best_acc)"
   ],
   "id": "702f1f2463b9bee8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=2e-05, train_bs=16, eval_bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 09:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.420025</td>\n",
       "      <td>0.769784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.429787</td>\n",
       "      <td>0.797212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.465635</td>\n",
       "      <td>0.792266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 00:19]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: lr=2e-05, train_bs=16, eval_bs=16 -> Accuracy: 0.7972\n",
      "Training with lr=2e-05, train_bs=16, eval_bs=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 09:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>0.425258</td>\n",
       "      <td>0.763040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.411995</td>\n",
       "      <td>0.798561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.461236</td>\n",
       "      <td>0.794514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:18]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: lr=2e-05, train_bs=16, eval_bs=32 -> Accuracy: 0.7986\n",
      "Training with lr=2e-05, train_bs=32, eval_bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 08:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.440266</td>\n",
       "      <td>0.756295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.401557</td>\n",
       "      <td>0.790468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>0.427917</td>\n",
       "      <td>0.785072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 00:19]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: lr=2e-05, train_bs=32, eval_bs=16 -> Accuracy: 0.7905\n",
      "Training with lr=2e-05, train_bs=32, eval_bs=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 08:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.438584</td>\n",
       "      <td>0.756295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.397647</td>\n",
       "      <td>0.790917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.424058</td>\n",
       "      <td>0.787320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:18]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: lr=2e-05, train_bs=32, eval_bs=32 -> Accuracy: 0.7909\n",
      "Training with lr=3e-05, train_bs=16, eval_bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 08:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.424321</td>\n",
       "      <td>0.766637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.399677</td>\n",
       "      <td>0.802158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.519337</td>\n",
       "      <td>0.801259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 00:19]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: lr=3e-05, train_bs=16, eval_bs=16 -> Accuracy: 0.8022\n",
      "Training with lr=3e-05, train_bs=16, eval_bs=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 08:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.426446</td>\n",
       "      <td>0.767536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.408493</td>\n",
       "      <td>0.801709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.518688</td>\n",
       "      <td>0.799460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:18]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: lr=3e-05, train_bs=16, eval_bs=32 -> Accuracy: 0.8017\n",
      "Training with lr=3e-05, train_bs=32, eval_bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 08:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.447674</td>\n",
       "      <td>0.755396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.410667</td>\n",
       "      <td>0.785971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.431585</td>\n",
       "      <td>0.788219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 00:19]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: lr=3e-05, train_bs=32, eval_bs=16 -> Accuracy: 0.7882\n",
      "Training with lr=3e-05, train_bs=32, eval_bs=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 08:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>0.434490</td>\n",
       "      <td>0.758543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.394251</td>\n",
       "      <td>0.794514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.437141</td>\n",
       "      <td>0.797212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:18]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: lr=3e-05, train_bs=32, eval_bs=32 -> Accuracy: 0.7972\n",
      "Best configuration: (3e-05, 16, 16) with accuracy: 0.802158273381295\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T03:16:52.645786Z",
     "start_time": "2025-04-08T03:07:25.703033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rebuild training arguments with the best hyperparameters from the grid search\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_output_final\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=best_config[0],\n",
    "    per_device_train_batch_size=best_config[1],\n",
    "    per_device_eval_batch_size=best_config[2],\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir=\"./logs_final\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Load a new BERT model instance\n",
    "final_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "final_model.to(device)\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "final_trainer.train()\n",
    "\n",
    "# Evaluate the final model on the dev set\n",
    "final_eval_results = final_trainer.evaluate()\n",
    "print(\"Final Evaluation Results on Dev Set:\")\n",
    "print(final_eval_results)"
   ],
   "id": "3e48e96623fcc60b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 09:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.410068</td>\n",
       "      <td>0.769335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.336300</td>\n",
       "      <td>0.407277</td>\n",
       "      <td>0.801709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.211100</td>\n",
       "      <td>0.527986</td>\n",
       "      <td>0.798561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 00:19]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Results on Dev Set:\n",
      "{'eval_loss': 0.4072774350643158, 'eval_accuracy': 0.8017086330935251, 'eval_runtime': 19.2906, 'eval_samples_per_second': 115.289, 'eval_steps_per_second': 7.206, 'epoch': 3.0}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T03:16:53.212699Z",
     "start_time": "2025-04-08T03:16:52.712202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_model.save_pretrained(\"Bert_Model_Final\")\n",
    "tokenizer.save_pretrained(\"Bert_Model_Final_Tokenizer\")\n",
    "\n",
    "import pickle\n",
    "target_folder = \"Compressed model folder\"  # already exists\n",
    "file_path = os.path.join(target_folder, \"Bert_Model_Final_State.pkl\")\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(final_model.state_dict(), f)\n",
    "\n",
    "print(\"Final BERT model and tokenizer saved in 'Bert_Model_Final', and state_dict pickled as 'Bert_Model_Final_State.pkl'.\")"
   ],
   "id": "b6f4f4cf9250c148",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final BERT model and tokenizer saved in 'Bert_Model_Final', and state_dict pickled as 'Bert_Model_Final_State.pkl'.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
