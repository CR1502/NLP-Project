{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import ssl\n",
    "import nltk\n",
    "\n",
    "# Set the NLTK_DATA environment variable to your provided path\n",
    "os.environ[\"NLTK_DATA\"] = \"/Users/craigroberts/Documents/Coding/NLP/MediScan_NLP_Proj/nltk_data\"\n",
    "\n",
    "# Add this directory to nltk's search path\n",
    "nltk.data.path.append(\"/Users/craigroberts/Documents/Coding/NLP/MediScan_NLP_Proj/nltk_data\")\n",
    "\n",
    "# Optionally disable SSL verification to avoid certificate errors when downloading\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"Setup complete. NLTK data path set to:\", os.environ[\"NLTK_DATA\"])"
   ],
   "id": "4459c5e3e560c563",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "1022d179506dc306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load your training and dev CSV files (adjust paths as needed)\n",
    "train_df = pd.read_csv(\"Final_data/train_data.csv\")  # Assumes columns 'claim' and 'label'\n",
    "dev_df   = pd.read_csv(\"Final_data/dev_data.csv\")\n",
    "\n",
    "print(\"Train DataFrame shape:\", train_df.shape)\n",
    "print(\"Dev DataFrame shape:\", dev_df.shape)\n",
    "\n",
    "# Encode labels if they are strings\n",
    "if train_df[\"label\"].dtype == object:\n",
    "    le = LabelEncoder()\n",
    "    all_labels = pd.concat([train_df[\"label\"], dev_df[\"label\"]], axis=0)\n",
    "    le.fit(all_labels)\n",
    "    train_df[\"label_encoded\"] = le.transform(train_df[\"label\"])\n",
    "    dev_df[\"label_encoded\"] = le.transform(dev_df[\"label\"])\n",
    "    num_labels = len(le.classes_)\n",
    "else:\n",
    "    train_df[\"label_encoded\"] = train_df[\"label\"]\n",
    "    dev_df[\"label_encoded\"] = dev_df[\"label\"]\n",
    "    num_labels = len(np.unique(train_df[\"label\"]))\n",
    "    class DummyLE:\n",
    "        pass\n",
    "    le = DummyLE()\n",
    "    le.classes_ = np.sort(np.unique(train_df[\"label\"]))\n",
    "\n",
    "print(\"Number of classes:\", num_labels)\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, range(num_labels))))\n",
    "\n",
    "# Create Hugging Face Datasets using 'claim' and 'label_encoded'\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"claim\", \"label_encoded\"]])\n",
    "dev_dataset = Dataset.from_pandas(dev_df[[\"claim\", \"label_encoded\"]])\n",
    "\n",
    "# Rename label column to \"labels\"\n",
    "train_dataset = train_dataset.rename_column(\"label_encoded\", \"labels\")\n",
    "dev_dataset = dev_dataset.rename_column(\"label_encoded\", \"labels\")\n",
    "\n",
    "# Remove extraneous columns (keep only 'claim' and 'labels')\n",
    "cols_to_keep = [\"claim\", \"labels\"]\n",
    "train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col not in cols_to_keep])\n",
    "dev_dataset = dev_dataset.remove_columns([col for col in dev_dataset.column_names if col not in cols_to_keep])\n",
    "\n",
    "print(\"Training samples:\", len(train_dataset), \"Dev samples:\", len(dev_dataset))"
   ],
   "id": "aa92589c75a9305a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the pre-trained BERT tokenizer (\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"claim\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the datasets in batches\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch (using only necessary columns)\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "dev_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ],
   "id": "d4deebd1a827bb24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = np.mean(preds == labels)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Base training arguments (will be updated in grid search)\n",
    "base_training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,                # Placeholder (to be tuned)\n",
    "    per_device_train_batch_size=16,    # Placeholder (to be tuned)\n",
    "    per_device_eval_batch_size=16,     # Placeholder (to be tuned)\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Load a fresh BERT model instance\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=base_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Trainer is set up!\")"
   ],
   "id": "6cc30de24095ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "# Define grid for hyperparameters\n",
    "learning_rates = [2e-5, 3e-5]\n",
    "train_batch_sizes = [16, 32]\n",
    "eval_batch_sizes = [16, 32]\n",
    "\n",
    "best_acc = 0\n",
    "best_config = None\n",
    "results = []\n",
    "\n",
    "# Manual grid search loop\n",
    "for lr, train_bs, eval_bs in itertools.product(learning_rates, train_batch_sizes, eval_batch_sizes):\n",
    "    print(f\"Training with lr={lr}, train_bs={train_bs}, eval_bs={eval_bs}\")\n",
    "\n",
    "    # Define training arguments for this configuration\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./temp_bert_output\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=train_bs,\n",
    "        per_device_eval_batch_size=eval_bs,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_dir=\"./temp_logs\",\n",
    "        logging_steps=50,\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Load a new model instance for each run\n",
    "    temp_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "    temp_model.to(device)\n",
    "\n",
    "    temp_trainer = Trainer(\n",
    "        model=temp_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    temp_trainer.train()\n",
    "    eval_results = temp_trainer.evaluate()\n",
    "    acc = eval_results[\"eval_accuracy\"]\n",
    "    results.append({\"learning_rate\": lr, \"train_bs\": train_bs, \"eval_bs\": eval_bs, \"accuracy\": acc})\n",
    "    print(f\"Configuration: lr={lr}, train_bs={train_bs}, eval_bs={eval_bs} -> Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_config = (lr, train_bs, eval_bs)\n",
    "\n",
    "print(\"Best configuration:\", best_config, \"with accuracy:\", best_acc)"
   ],
   "id": "702f1f2463b9bee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rebuild training arguments with the best hyperparameters from the grid search\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_output_final\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=best_config[0],\n",
    "    per_device_train_batch_size=best_config[1],\n",
    "    per_device_eval_batch_size=best_config[2],\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir=\"./logs_final\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Load a new BERT model instance\n",
    "final_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "final_model.to(device)\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "final_trainer.train()\n",
    "\n",
    "# Evaluate the final model on the dev set\n",
    "final_eval_results = final_trainer.evaluate()\n",
    "print(\"Final Evaluation Results on Dev Set:\")\n",
    "print(final_eval_results)"
   ],
   "id": "3e48e96623fcc60b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the final BERT model and tokenizer using Hugging Face's save_pretrained method\n",
    "final_model.save_pretrained(\"Bert_Model_Final\")\n",
    "tokenizer.save_pretrained(\"Bert_Model_Final_Tokenizer\")\n",
    "\n",
    "# Optionally, pickle the model's state_dict\n",
    "import pickle\n",
    "with open(\"Bert_Model_Final_State.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model.state_dict(), f)\n",
    "\n",
    "print(\"Final BERT model and tokenizer saved in 'Bert_Model_Final', and state_dict pickled as 'Bert_Model_Final_State.pkl'.\")"
   ],
   "id": "b6f4f4cf9250c148",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9e1c5db5eaad7afd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
